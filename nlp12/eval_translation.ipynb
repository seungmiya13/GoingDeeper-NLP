{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bright-freeware",
   "metadata": {},
   "source": [
    "GoingDeeper-NLP 12. 번역가는 대화에도 능하다\n",
    "===\n",
    "\n",
    "다양한 디코딩 방식을 활용해 번역 모델을 직접 만들어 본 후, 해당 모델을 BLEU Score을 이용해 성능을 평가해 본다. 트랜스포머 구조를 활용해 한국어 챗봇을 직접 구현해 보는 프로젝트를 진행한다.\n",
    "\n",
    "[학습 목차]\n",
    "- 12-2. 번역 모델 만들기\n",
    "- 12-3. 번역 성능 측정하기 (1) BLEU Score\n",
    "- 12-4. 번역 성능 측정하기 (2) Beam Search Decoder\n",
    "- 12-5. 데이터 부풀리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-retreat",
   "metadata": {},
   "source": [
    "# 12-2. 번역 모델 만들기\n",
    "\n",
    "먼저 번역 모델이 있어야 챗봇을 만들 수 있겠죠? 이번 실습에선 접근성이 좋은 영어-스페인어 데이터를 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-silence",
   "metadata": {},
   "source": [
    "## 라이브러리와 데이터 준비하기 \n",
    "\n",
    "필요한 라이브러리를 import 한 후, 아래 소스를 실행해 데이터를 다운로드 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respective-variance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imperial-alberta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 0s 0us/step\n",
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrázame.\n",
      ">> No way!\t¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(corpus))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in corpus[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-leisure",
   "metadata": {},
   "source": [
    "이번엔 한-영 번역 때와 다르게, 두 언어가 단어 사전을 공유하도록 하겠습니다. 영어와 스페인어 모두 알파벳으로 이뤄지는 데다가 같은 인도유럽어족이기 때문에 기대할 수 있는 효과가 많아요! 후에 챗봇을 만들 때에도 질문과 답변이 모두 한글로 이루어져 있기 때문에 Embedding 층을 공유하는 것이 성능에 도움이 됩니다.\n",
    "     \n",
    "토큰화에는 Sentencepiece를 사용할 것이고 단어 사전 수는 20,000으로 설정하겠습니다.\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-beads",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "\n",
    "중복 데이터를 set 데이터형을 활용해 제거한 후, Sentencepiece 기반의 토크나이저를 생성해 주는 generate_tokenizer() 함수를 정의하여 토크나이저를 얻어보도록 하죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interior-receipt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = list(set(corpus)) # 중복 제거 \n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(cleaned_corpus, VOCAB_SIZE)\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-uncertainty",
   "metadata": {},
   "source": [
    "위에서 두 언어 사이에 단어 사전을 공유하기로 하였으므로, 따라서 Encoder와 Decoder의 전용 토크나이저를 만들지 않고, 방금 만들어진 토크나이저를 두 언어 사이에서 공유하게 됩니다.\n",
    "   \n",
    "토크나이저가 준비되었으니 본격적으로 데이터를 토큰화하도록 하겠습니다. 문장부호와 대소문자 등을 정제하는 preprocess_sentence() 함수를 정의해 데이터를 정제하고, 정제된 데이터가 **50개 이상의 토큰을 갖는 경우 제거**하도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "technical-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "engaging-frank",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef6b6f5eaa24d108d724d086f9fe654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "118951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for pair in tqdm_notebook(cleaned_corpus):\n",
    "    src, tgt = pair.split('\\t')\n",
    "\n",
    "    src_tokens = tokenizer.encode_as_ids(preprocess_sentence(src))   # encode_as_ids() 는 문자열을 숫자로 분할\n",
    "    tgt_tokens = tokenizer.encode_as_ids(preprocess_sentence(tgt))\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-methodology",
   "metadata": {},
   "source": [
    "list 자료형도 단숨에 패딩 작업을 해주는 멋진 함수 pad_sequences() 를 기억하시죠? 단숨에 데이터셋을 완성하도록 하겠습니다! 아 참, 그리고 다음 스텝에서 활용할 예정이니 **딱 1%의 데이터만 테스트셋**으로 빼놓을게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wireless-fairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train : 117761 enc_val : 1190\n",
      "dec_train : 117761 dec_val : 1190\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(\"enc_train :\", len(enc_train), \"enc_val :\", len(enc_val))\n",
    "print(\"dec_train :\", len(dec_train), \"dec_val :\",len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-level",
   "metadata": {},
   "source": [
    "## 트랜스포머 구현하기\n",
    "\n",
    "생성된 데이터를 학습할 수 있는 멋진 트랜스포머(Transformer)를 구현하세요!\n",
    "    \n",
    "트랜스포머 구조가 잘 기억나지 않으시거나 구현에 도움이 필요하시면 아래 링크를 참고해 주세요. 트랜스포머 구조 참고 자료와 PyTorch로 구현이 되어있지만, 상세히 설명되어 있는 블로그를 소개해드리겠습니다.\n",
    "\n",
    "- 기본 구조 참고: [위키독스: 트랜스포머](https://wikidocs.net/31379)\n",
    "- PyTorch로 구현된 트랜스포머(1): [Transformer (Attention Is All You Need) 구현하기 (1/3)](https://paul-hyun.github.io/transformer-01/)\n",
    "- PyTorch로 구현된 트랜스포머(2): [Transformer (Attention Is All You Need) 구현하기 (2/3)](https://paul-hyun.github.io/transformer-02/)\n",
    "- PyTorch로 구현된 트랜스포머(3): [Transformer (Attention Is All You Need) 구현하기 (3/3)](https://paul-hyun.github.io/transformer-03/)\n",
    "- Attention Layer 구현: [Transformer with Python and TensorFlow 2.0 – Attention Layers](https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/)\n",
    "\n",
    "단, Encoder와 Decoder 각각의 Embedding과 출력층의 Linear, 총 3개의 레이어가 Weight를 공유할 수 있게 합니다\n",
    "\n",
    "하이퍼파라미터는 아래와 동일하게 정의합니다.\n",
    "    \n",
    "- transformer = Transformer(    \n",
    "    n_layers=2,<br>\n",
    "    d_model=512,<br>\n",
    "    n_heads=8,<br>\n",
    "    d_ff=2048,<br>\n",
    "    src_vocab_size=VOCAB_SIZE,<br>\n",
    "    tgt_vocab_size=VOCAB_SIZE,<br>\n",
    "    pos_len=200,<br>\n",
    "    dropout=0.3,<br>\n",
    "    shared_fc=True,<br>\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-memorial",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "third-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-marking",
   "metadata": {},
   "source": [
    "## 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "south-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src) # padding mask \n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "    \n",
    "    # 룩-어헤드 마스크(look-ahead mask)\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask) # padding + lookahead mask \n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask) # padding + lookahead mask\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-upper",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "naked-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-resident",
   "metadata": {},
   "source": [
    "## Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "occasional-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-baseball",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "revised-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-exception",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "painted-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-paste",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "horizontal-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-maximum",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "french-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-mainland",
   "metadata": {},
   "source": [
    "## Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "military-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-registration",
   "metadata": {},
   "source": [
    "## 모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "intimate-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-satellite",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "environmental-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-terminal",
   "metadata": {},
   "source": [
    "## Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aggregate-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-society",
   "metadata": {},
   "source": [
    "## Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "round-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-delivery",
   "metadata": {},
   "source": [
    "## Train Step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "micro-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-michigan",
   "metadata": {},
   "source": [
    "## 훈련을 시키자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "opening-glasgow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687572b739004ec198aa988e208d5704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fbea3d7d4a49e5ada9e0e0e28f4e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd6e84c21264a62ac95a328c86d7772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련시키기\n",
    "\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-inventory",
   "metadata": {},
   "source": [
    "# 12-3. 번역 성능 측정하기 (1) BLEU Score\n",
    "\n",
    "번역 모델을 훈련한 김에 라이브러리를 활용해서 간단하게 BLEU Score를 실습해 보겠습니다!\n",
    "\n",
    "- [BLEU Score](https://donghwa-kim.github.io/BLEU.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-yeast",
   "metadata": {},
   "source": [
    "## NLTK를 활용한 BLEU Score\n",
    "\n",
    "NLTK는 Natural Language Tool Kit 의 준말로 이름부터 자연어 처리에 큰 도움이 될 것 같은 라이브러리입니다. nltk 가 BLEU Score를 지원하니 이를 활용 합니다. nltk 가 설치되어 있지 않다면 pip install nltk 로 간단하게 설치할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "framed-working",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk # nltk가 설치되어 있지 않은 경우 주석 해제\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "finnish-metro",
   "metadata": {},
   "source": [
    "BLEU Score는 0~1 사이의 값을 가지지만, 100을 곱한 백분율 값으로 표기하는 경우도 많습니다. BLEU Score의 점수대별 해석에 대해서는 [여기](https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu)를 참고 합니다.\n",
    "\n",
    "BLEU Score가 50점을 넘는다는 것은 정말 멋진 번역을 생성했다는 의미예요. 보통 논문에서 제시하는 BLEU Score는 20점에서 높으면 40점을 바라보는 정도거든요! 하지만 방금 나온 점수는 사실상 0점이라고 해야 하겠네요. 그렇게까지 엉망진창인 번역이 된 것일까요?\n",
    "    \n",
    "BLEU Score의 정의로 돌아가 한번 따져봅시다. BLEU Score가 N-gram으로 점수를 측정한다는 것을 기억하실 거예요. \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "1-gram부터 4-gram까지의 점수(Precision)를 모두 곱한 후, 루트를 두 번 씌우면(^1/4) BLEU Score가 됩니다. 진정 멋진 번역이라면, 모든 N-gram에 대해서 높은 점수를 얻었을 거예요. \n",
    "    \n",
    "그렇다면 위에서 살펴본 예시에서는 각 N-gram이 점수를 얼마나 얻었는지 확인해 보도록 합니다. weights의 디폴트 값은 [0.25, 0.25, 0.25, 0.25]로 1-gram부터 4-gram까지의 점수에 가중치를 동일하게 주는 것이지만, 만약 이 값을 [1, 0, 0, 0]으로 바꿔주면 BLEU Score에 1-gram의 점수만 반영하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "interesting-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "prostate-third",
   "metadata": {},
   "source": [
    "0점에 가까운 BLEU Score가 나오는 원인을 알 수 있겠네요. 바로 3-gram와 4-gram에서 거의 0점을 받았기 때문인데요, 위 예시에서 번역문 문장 중 어느 3-gram도 원문의 3-gram과 일치하는 것이 없기 때문입니다. 2-gram이 0.18이 나오는 것은 원문의 11개 2-gram 중에 2개만이 번역문에서 재현되었기 때문입니다.\n",
    "    \n",
    "하지만 만약 nltk의 낮은 버전을 사용할 경우, 간혹 이런 경우에 3-gram, 4-gram 점수가 1이 나와서, 전체적인 BLEU 점수가 50점 이상으로 매우 높게 나오게 될 수도 있습니다.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "예전 버전에서는 위 수식에서 어떤 N-gram이 0의 값을 갖는다면 그 하위 N-gram 점수들이 곱했을 때 모두 소멸해버리기 때문에 일치하는 N-gram이 없더라도 점수를 1.0 으로 유지하여 하위 점수를 보존하게끔 구현되어 있었습니다. 하지만 1.0 은 모든 번역을 완벽히 재현했음을 의미하기 때문에 총점이 의도치 않게 높아질 수 있어요! 그럴 경우에는 BLEU Score가 바람직하지 못할 것(Undesirable)이라는 경고문이 추가되긴 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-testament",
   "metadata": {},
   "source": [
    "## SmoothingFunction()으로 BLEU Score 보정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-blend",
   "metadata": {},
   "source": [
    "그래서 BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용하고 있습니다. Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어지죠. 즉 우리의 의도대로 점수가 계산되는 거예요.\n",
    "\n",
    "진실된 BLEU Score를 확인하기 위해 어서 SmoothingFunction() 을 적용해 봅시다! 아래 코드에서는 SmoothingFunction().method1을 사용해 보겠습니다. 자신만의 Smoothing 함수를 구현해서 적용할 수도 있겠지만, nltk에서는 method0부터 method7까지를 이미 제공하고 있습니다.\n",
    "\n",
    "- (참고) 각 method들의 상세한 설명은 [nltk의 bleu_score 소스코드](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)를 참고해 봅시다. sentence_bleu() 함수에 smoothing_function=None을 적용하면 method0가 기본 적용됨을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "burning-tactics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-forth",
   "metadata": {},
   "source": [
    "SmoothingFunction()로 BLEU score를 보정한 결과, 새로운 BLEU 점수는 무려, 5점으로 올라갔습니다. 거의 의미 없는 번역이라는 냉정한 평가를 받게 되는군요.\n",
    "    \n",
    "여기서 BLEU-4가 BLEU-3보다 조금이나마 점수가 높은 이유는 한 문장에서 발생하는 3-gram 쌍의 개수와 4-gram 쌍의 개수를 생각해 보면 이해할 수 있습니다. 각 Precision을 N-gram 개수로 나누는 부분에서 차이가 발생하는 것이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-model",
   "metadata": {},
   "source": [
    "## 트랜스포머 모델의 번역 성능 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-suspect",
   "metadata": {},
   "source": [
    "위 예시를 조금만 응용하면 우리가 훈련한 모델이 얼마나 번역을 잘하는지 평가할 수 있습니다! 아까 1%의 데이터를 테스트셋으로 빼 둔 것을 기억하시죠? 테스트셋으로 모델의 BLEU Score를 측정하는 함수 eval_bleu() 를 구현해보도록 합시다!\n",
    "\n",
    "먼저 번역을 생성하기 위해 evaluate() 함수와 translate() 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "republican-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)  # 문자열을 token으로 분할 \n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)  # 문자열을 숫자로 분할\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()  # predictions에 소프트맥스 함수를 적용하여 가장 큰 값의 인덱스를 predicted_id로 저장\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)  # 숫자를 문자열로 복원합니다.  \n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)  \n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-motel",
   "metadata": {},
   "source": [
    "- tf.expand_dims(input, axis) :    \n",
    "    Returns a tensor with a length 1 axis inserted at index axis    \n",
    "    [link](https://www.tensorflow.org/api_docs/python/tf/expand_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sustained-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-championship",
   "metadata": {},
   "source": [
    "이제 함수 eval_bleu() 를 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "electronic-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "\n",
    "        src_sentence = tokenizer.decode_ids((src_tokens.tolist()))  \n",
    "        tgt_sentence = tokenizer.decode_ids((tgt_tokens.tolist()))\n",
    "\n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer).split()\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-cream",
   "metadata": {},
   "source": [
    "eval_bleu() 또한 아주 어려운 내용은 없습니다. 주어진 병렬 말뭉치 src_corpus 와 tgt_corpus 를 인덱스 순으로 살피며 소스 토큰과 타겟 토큰을 각각 원문으로 Decoding 하고, 소스 문장을 translate() 함수를 통해 번역한 후 생성된 번역문과 타겟 문장의 BLEU Score를 측정합니다. 측정된 score 는 total_score 에 합산되어 최종적으로 주어진 병렬 말뭉치의 평균 BLEU Score를 출력하죠!\n",
    "\n",
    "verbose 변수를 True 로 주면 번역문과 원문, 매 스텝의 점수를 확인할 수 있습니다. 간단히 동작시켜볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecological-worth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6846194562ad401c88e50890be81c295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  the atomic number for iron is ......................................\n",
      "Model Prediction:  ['s', ',', 's', ',', 's', 's']\n",
      "Real:  ['el', 'n', 'mero', 'at', 'mico', 'del', 'hierro', 'es', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  i was hungry and thirsty ........................................\n",
      "Model Prediction:  ['s', ',', 's', ',', 's', 's']\n",
      "Real:  ['ten', 'a', 'hambre', 'y', 'sed', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Source Sentence:  he inherited the business from his father .....................................\n",
      "Model Prediction:  ['s', ',', 's', ',', 's', 's']\n",
      "Real:  ['l', 'hered', 'el', 'negocio', 'de', 'su', 'padre', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-envelope",
   "metadata": {},
   "source": [
    "3 Epoch밖에 학습하지 않아서인지 성능이 좋지 않네요. 표본이 적은 것일 수도 있으니 좀 더 많은 데이터로 측정해 보겠습니다.\n",
    "\n",
    "전체 테스트셋으로 측정하는 것은 시간이 제법 걸리기 때문에 1/10만 사용해서 실습하는 걸 권장할게요. enc_val[::10] 의 [::10] 은 리스트를 10개씩 건너뛰어 추출하라는 의미로 지금 적용하기에 딱 맞는 문법이죠? 출력문 지옥을 피하고 싶으시다면 verbose 를 False 로 설정하는 것도 잊지 마세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eastern-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa9352b9b0543c58715eaba84b38208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 119\n",
      "Total Score: 5.143880389332569e-05\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-postage",
   "metadata": {},
   "source": [
    "# 12-4. 번역 성능 측정하기 (2) Beam Search Decoder\n",
    "\n",
    "이 멋진 평가 지표를 더 멋지게 사용하는 방법! 바로 모델의 생성 기법에 변화를 주는 것이죠. Greedy Decoding 대신 새로운 기법을 적용하면 우리 모델을 더 잘 평가할 수 있을 것 같네요!\n",
    "    \n",
    "Beam Search를 기억하나요? 예시로 활용했던 코드를 다시 한번 살펴보면,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unnecessary-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sapphire-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-sustainability",
   "metadata": {},
   "source": [
    "사실 이 예시는 Beam Search를 설명하는 데에는 더없이 적당하지만 실제로 모델이 문장을 생성하는 과정과는 거리가 멉니다. 당장 모델이 문장을 생성하는 과정만 떠올려도 위의 prob_seq 처럼 확률을 정의할 수 없겠다는 생각이 머리를 스치죠. 각 단어에 대한 확률은 prob_seq 처럼 한 번에 정의가 되지 않고 이전 스텝까지의 단어에 따라서 결정되기 때문입니다!\n",
    "\n",
    "간단한 예시로, Beam Size가 2이고 Time-step이 2인 순간의 두 문장이 나는 밥을 , 나는 커피를 이라고 한다면 세 번째 단어로 먹는다 , 마신다 를 고려할 수 있습니다. 이때, 전자에서 마신다 에 할당하는 확률과 후자에서 마신다 에 할당하는 확률은 각각 이전 단어들인 나는 밥을 , 나는 커피를 에 따라서 결정되기 때문에 서로 독립적인 확률을 갖습니다. 예컨대 후자가 마신다 에 더 높은 확률을 할당할 것을 알 수 있죠! 위 소스에서처럼 \"3번째 단어는 항상 [마신다: 0.3, 먹는다:0.5, ...] 의 확률을 가진다!\" 라고는 할 수 없다는 겁니다.\n",
    "\n",
    "따라서 Beam Search를 생성 기법으로 구현할 때에는 분기를 잘 나눠줘야 합니다. Beam Size가 5라고 가정하면 맨 첫 단어로 적합한 5개의 단어를 생성하고, 두 번째 단어로 각 첫 단어(5개 단어)에 대해 5순위까지 확률을 구하여 총 25개의 문장을 생성하죠. 그 25개의 문장들은 각 단어에 할당된 확률을 곱하여 구한 점수(존재 확률)를 가지고 있으니 각각의 순위를 매길 수 있겠죠? 점수 상위 5개의 표본만 살아남아 세 번째 단어를 구할 자격을 얻게 됩니다.\n",
    "\n",
    "위 과정을 반복하면 최종적으로 점수가 가장 높은 5개의 문장을 얻게 됩니다. 물론 Beam Size를 조절해 주면 그 수는 유동적으로 변할 거구요! 다들 잘 이해하셨죠? 😃"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-ultimate",
   "metadata": {},
   "source": [
    "# Beam Search Decoder 작성 및 평가하기\n",
    "\n",
    "각 단어의 확률값을 계산하는 calc_prob()와 Beam Search를 기반으로 동작하는 beam_search_decoder() 를 구현하고 생성된 문장에 대해 BLEU Score를 출력하는 beam_bleu() 를 구현합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fallen-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_prob() 구현\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "    \n",
    "    return tf.math.softmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rough-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_decoder() 구현\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, tgt_len), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "controlling-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "# beam_bleu() 구현\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-movement",
   "metadata": {},
   "source": [
    "구현 후 다음과 같이 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "prostate-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['tuve', 'una', 'larga', 'conversaci', 'n', 'con', 'tom', '...................................']\n",
      "Candidate: ['s', ',', 's', ',', 's', 's']\n",
      "BLEU: 0\n",
      "Reference: ['tuve', 'una', 'larga', 'conversaci', 'n', 'con', 'tom', '...................................']\n",
      "Candidate: ['s', ',', 's', ',', 's']\n",
      "BLEU: 0\n",
      "Reference: ['tuve', 'una', 'larga', 'conversaci', 'n', 'con', 'tom', '...................................']\n",
      "Candidate: ['s', ',', 's', ',', 's', 'ss']\n",
      "BLEU: 0\n",
      "Reference: ['tuve', 'una', 'larga', 'conversaci', 'n', 'con', 'tom', '...................................']\n",
      "Candidate: ['s', ',', 's', ',', 's', 's']\n",
      "BLEU: 0\n",
      "Reference: ['tuve', 'una', 'larga', 'conversaci', 'n', 'con', 'tom', '...................................']\n",
      "Candidate: ['s', ',', 's', ',', 's', 's', 's']\n",
      "BLEU: 0\n"
     ]
    }
   ],
   "source": [
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-acceptance",
   "metadata": {},
   "source": [
    "# 12-5. 데이터 부풀리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-architect",
   "metadata": {},
   "source": [
    "이번 스텝에서는 Data Augmentation, 그중에서도 Embedding을 활용한 Lexical Substitution을 구현해 볼 거예요. gensim 라이브러리를 활용하면 어렵지 않게 해낼 수 있습니다!\n",
    "    \n",
    "컴퓨터에 gensim이 설치되어 있지 않은 경우, 먼저 아래 명령어를 실행해 gensim 을 설치해 주세요.\n",
    "- $ pip install gensim\n",
    "\n",
    "gensim 에 사전 훈련된 Embedding 모델을 불러오는 것은 두 가지 방법이 있습니다.\n",
    "\n",
    "1) 직접 모델을 다운로드해 load 하는 방법    \n",
    "2) gensim 이 자체적으로 지원하는 downloader 를 활용해 모델을 load 하는 방법\n",
    "\n",
    "한국어는 gensim 에서 지원하지 않으므로 두 번째 방법을 사용할 수 없지만, 영어라면 얘기가 달라지죠! 아래 웹페이지의 Available data → Model 부분에서 공개된 모델의 종류를 확인할 수 있습니다.\n",
    "- [RaRe-Technologies/gensim-data](https://github.com/RaRe-Technologies/gensim-data)\n",
    "    \n",
    "대표적으로 사용되는 Embedding 모델은 word2vec-google-news-300 이지만 용량이 커서 다운로드에 많은 시간이 소요되므로 이번 실습엔 적합하지 않습니다. 우리는 적당한 사이즈의 모델인 glove-wiki-gigaword-300 을 사용할게요! 아래 소스를 실행해 사전 훈련된 Embedding 모델을 다운로드해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "olive-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-canadian",
   "metadata": {},
   "source": [
    "불러온 모델은 아래와 같이 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pregnant-disposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462778806686401),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.52181077003479),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.48399588465690613),\n",
       " ('peanut', 0.48062023520469666),\n",
       " ('potato', 0.48061180114746094)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-switch",
   "metadata": {},
   "source": [
    "주어진 데이터를 토큰 단위로 분리한 후, 랜덤하게 하나를 선정하여 해당 토큰과 가장 유사한 단어를 찾아 대치하면 그것으로 **Lexical Substitution**은 완성되겠죠? 가볍게 확인해 봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "activated-george",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: you know ? those you need is attention . \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-appliance",
   "metadata": {},
   "source": [
    "## Lexical Substitution 구현하기\n",
    "\n",
    "입력된 문장을 Embedding 유사도를 기반으로 Augmentation 하여 반환하는 lexical_sub() 를 구현하세요!\n",
    "\n",
    "그리고 구현한 함수를 활용해 3,000개의 영문 데이터를 Augmentation 하고 결과를 확인하세요!\n",
    "\n",
    "단어장에 포함되지 않은 단어가 들어오는 경우, 문장부호에 대한 치환이 발생하는 경우 등의 예외는 자유롭게 처리하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "transparent-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "    \n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hidden-healing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f556ae766be543b6874c5bc672996867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i ll be very busy next month , ', 'i ll be very busy next month .', 'they re smiling at each other , ', 'they re smiling at each other .', 'who is in the car ? tom now . ', 'who is in the car ? tom is .', 'but s theirs . ', 'that s theirs .', \"'d d just like to talk with tom before we leave . \", 'i d just like to talk with tom before we leave .']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(3000)):\n",
    "    old_src = tokenizer.decode_ids(src_corpus[idx])\n",
    "\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "\n",
    "    if new_src is not None: new_corpus.append(new_src)\n",
    "\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
