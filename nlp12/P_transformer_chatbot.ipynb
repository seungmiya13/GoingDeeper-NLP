{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "environmental-sheffield",
   "metadata": {},
   "source": [
    "Project : 멋진 챗봇 만들기\n",
    "===\n",
    "\n",
    "트랜스포머 구조를 활용해 한국어 챗봇을 직접 구현해 보는 프로젝트를 진행한다. 또한 번역기 성능 측정법을 챗봇에도 적용해 봅니다. \n",
    "\n",
    "[목차]\n",
    "1. 데이터 다운로드\n",
    "2. 데이터 정제\n",
    "3. 데이터 토큰화\n",
    "4. Augmentation\n",
    "5. 데이터 벡터화\n",
    "6. 훈련하기\n",
    "7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complimentary-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-lecture",
   "metadata": {},
   "source": [
    "# 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-virus",
   "metadata": {},
   "source": [
    "아래 링크에서 ChatbotData.csv 를 다운로드해 챗봇 훈련 데이터를 확보합니다. pandas 라이브러리로 csv 파일을 읽어봅니다. 읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장합니다. \n",
    "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "structural-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ChatbotData .csv'   ko.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/aiffel/GoingDeeper/nlp12/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spectacular-federal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('~/aiffel/GoingDeeper/nlp12/data/ChatbotData .csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spread-chester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "peaceful-nickel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A\n",
       "0           12시 땡!   하루가 또 가네요.\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.\n",
       "4          PPL 심하네   눈살이 찌푸려지죠."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop((\"label\"), axis=\"columns\", inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-childhood",
   "metadata": {},
   "source": [
    "# 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-police",
   "metadata": {},
   "source": [
    "## 중복 제거\n",
    "먼저, 중복 데이터를 확인하고 제거해 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aging-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>결혼이나 하지 왜 자꾸 나한테 화 내냐구!</td>\n",
       "      <td>힘들겠네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>고백하고 후회하면 어떡하지</td>\n",
       "      <td>후회는 후회를 낳을뿐이에요. 용기 내세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>공부는 내 체질이 아닌 것 같아</td>\n",
       "      <td>확신이 없나봐요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>기숙사 괜찮을까</td>\n",
       "      <td>혼자 사는 것보다 불편하겠죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5774</th>\n",
       "      <td>나는 좋은데 ….</td>\n",
       "      <td>현실의 벽에 부딪혔나봐요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8764</th>\n",
       "      <td>환승 가능?</td>\n",
       "      <td>환승은 30분 안에</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8780</th>\n",
       "      <td>회사 사람들이 아직도 불편해</td>\n",
       "      <td>회사에는 동료가 있을 뿐이에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>회사에는 왜 친구 같은 사람이 없을까</td>\n",
       "      <td>회사는 친구 사귀는 곳이 아니에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8789</th>\n",
       "      <td>후련하달까</td>\n",
       "      <td>후련하니 다행이에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>내일 만나자고 해볼까?</td>\n",
       "      <td>멋지게 데이트 신청 해보세요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Q                        A\n",
       "5527  결혼이나 하지 왜 자꾸 나한테 화 내냐구!                   힘들겠네요.\n",
       "5537           고백하고 후회하면 어떡하지  후회는 후회를 낳을뿐이에요. 용기 내세요.\n",
       "5542        공부는 내 체질이 아닌 것 같아                확신이 없나봐요.\n",
       "5704                 기숙사 괜찮을까         혼자 사는 것보다 불편하겠죠.\n",
       "5774                나는 좋은데 ….           현실의 벽에 부딪혔나봐요.\n",
       "...                       ...                      ...\n",
       "8764                   환승 가능?               환승은 30분 안에\n",
       "8780          회사 사람들이 아직도 불편해        회사에는 동료가 있을 뿐이에요.\n",
       "8782     회사에는 왜 친구 같은 사람이 없을까      회사는 친구 사귀는 곳이 아니에요.\n",
       "8789                    후련하달까              후련하니 다행이에요.\n",
       "9541             내일 만나자고 해볼까?         멋지게 데이트 신청 해보세요.\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afraid-thirty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "original-recognition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750 11750\n"
     ]
    }
   ],
   "source": [
    "questions, answers = list(data['Q']), list(data['A'])\n",
    "\n",
    "print(len(questions), len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-participant",
   "metadata": {},
   "source": [
    "## 전처리 함수 \n",
    "\n",
    "이제, 아래 조건을 만족하는 preprocess_sentence() 함수를 구현합니다.\n",
    "\n",
    "1. 영문자의 경우, 모두 소문자로 변환합니다.\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다.\n",
    "   \n",
    "문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stopped-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^0-9가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-hacker",
   "metadata": {},
   "source": [
    "# 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-flush",
   "metadata": {},
   "source": [
    "토큰화에는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "- [Mecab Class](https://konlpy.org/ko/latest/api/konlpy.tag/#mecab-class)\n",
    "\n",
    "## 토큰화 함수\n",
    "\n",
    "아래 조건을 만족하는 build_corpus() 함수를 구현합니다. \n",
    "\n",
    "1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.\n",
    "3. 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달합니다.\n",
    "4. 중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의합니다. \n",
    "\n",
    "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cooperative-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src, tgt, tokenizer):\n",
    "    \n",
    "    src_corpus = []\n",
    "    tgt_corpus = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(src)):\n",
    "        src_sen = preprocess_sentence(src[i])\n",
    "        tgt_sen = preprocess_sentence(tgt[i])\n",
    "\n",
    "        src_tokens = tokenizer.morphs(src_sen)\n",
    "        tgt_tokens = tokenizer.morphs(tgt_sen)\n",
    "        \n",
    "        if src_tokens in src_corpus and tgt_tokens in tgt_corpus: continue\n",
    "        \n",
    "        src_corpus.append(src_tokens)\n",
    "        tgt_corpus.append(tgt_tokens)\n",
    "        \n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "combined-reunion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11719 11719\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers, tokenizer)\n",
    "print(len(que_corpus), len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bulgarian-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네']]\n",
      "[['하루', '가', '또', '가', '네요', '.'], ['위로', '해', '드립니다', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['눈살', '이', '찌푸려', '지', '죠', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[:5])\n",
    "print(ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-rouge",
   "metadata": {},
   "source": [
    "## 토큰 개수 제한\n",
    "\n",
    "토큰 개수 분포를 확인하여 토큰의 개수가 일정 길이 이상은 데이터에서 제외합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "posted-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰개수 평균 :  7.7065449270415565\n",
      "토큰개수 최대 :  40\n",
      "토큰개수 표준편차 :  3.6117207348364158\n"
     ]
    }
   ],
   "source": [
    "total_data = list(que_corpus) + list(ans_corpus)\n",
    "\n",
    "num_tokens = [len(tokens) for tokens in total_data]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 토큰개수의 평균값, 최대값, 표준편차 계산\n",
    "print('토큰개수 평균 : ', np.mean(num_tokens))\n",
    "print('토큰개수 최대 : ', np.max(num_tokens))\n",
    "print('토큰개수 표준편차 : ', np.std(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "serial-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_sequences maxlen :  16\n",
      "전체 데이터의 0.9756805188155986%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "# 최대 길이를 (평균 + 2.3*표준편차)로 지정\n",
    "max_tokens = np.mean(num_tokens) + 2.3 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 데이터의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-practice",
   "metadata": {},
   "source": [
    "토큰의 개수가 16개 이상인 데이터는 제외시켜 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fluid-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11058 11058\n"
     ]
    }
   ],
   "source": [
    "que_corpus2 = []\n",
    "ans_corpus2 = []\n",
    "\n",
    "for i in range(len(que_corpus)):\n",
    "    if len(que_corpus[i]) < maxlen or len(ans_corpus[i]) < maxlen: \n",
    "        que_corpus2.append(que_corpus[i])\n",
    "        ans_corpus2.append(ans_corpus[i])\n",
    "        \n",
    "print(len(que_corpus2), len(ans_corpus2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 1%는 test 데이터로 남겨둡니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-blind",
   "metadata": {},
   "source": [
    "# 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-entity",
   "metadata": {},
   "source": [
    "우리에게 주어진 데이터는 1만 개가량으로 적은 편에 속합니다. 이럴 때에 사용할 수 있는 Lexical Substitution을 실제로 적용해 봅니다. \n",
    "    \n",
    "아래 링크를 참고하여 한국어로 사전 훈련된 Embedding 모델을 다운로드합니다. Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하고, ko.bin 파일을 얻습니다. \n",
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "    \n",
    "다운로드한 모델을 활용해 데이터를 Augmentation 합니다.\n",
    "    \n",
    "1) Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록 합니다.     \n",
    "2) 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 합니다.  \n",
    "\n",
    "위의 과정으로 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sublime-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_path = os.getenv('HOME')+'/aiffel/GoingDeeper/nlp12/data/ko.bin'\n",
    "w2v = gensim.models.Word2Vec.load(w2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "automotive-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(sentence)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "        \n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in sentence:\n",
    "        if tok is _from: res.append(_to)\n",
    "        else: res.append(tok)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "surprised-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11058 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████| 11058/11058 [00:19<00:00, 575.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '시', '땡', '캐치'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '김', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '겠', '다'], ['ppl', '심하', '네'], ['sd', '카드', '망가졌', '어'], ['sd', '카드', '못가', '돼'], ['sns', '들어맞', '팔', '왜', '안', '하', '지'], ['sns', '시간', '낭비', '인', '거', '아', '으며', '매일', '하', '는', '중'], ['sns', '시간', '낭비', '인데', '자꾸', '살펴보', '게', '됨']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 1) que_corpus Augmentation\n",
    "new_que = []\n",
    "\n",
    "for idx in tqdm(range(len(que_corpus2))):\n",
    "    old_src = que_corpus2[idx]\n",
    "\n",
    "    new_src = lexical_sub(old_src, w2v)\n",
    "\n",
    "    if new_src is not None: new_que.append(new_src)\n",
    "    else: new_que.append(old_src)\n",
    "\n",
    "print(new_que[:10])\n",
    "print(len(new_que))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "drawn-friendship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11058"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "miniature-shift",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9344 9344\n",
      "[['12', '시', '땡', '캐치'], ['3', '김', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '겠', '다'], ['sd', '카드', '못가', '돼'], ['sns', '들어맞', '팔', '왜', '안', '하', '지']]\n",
      "[['하루', '가', '또', '가', '네요', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['다시', '새로', '사', '는', '게', '마음', '편해요', '.'], ['잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록 \n",
    "que_corpus3 = []\n",
    "ans_corpus3 = []\n",
    "\n",
    "for i in range(len(new_que)): \n",
    "    if new_que[i] not in que_corpus2: # 중복 제거 \n",
    "        que_corpus3.append(new_que[i])\n",
    "        ans_corpus3.append(ans_corpus2[i])\n",
    "        \n",
    "print(len(que_corpus3), len(ans_corpus3))\n",
    "print(que_corpus3[:5])\n",
    "print(ans_corpus3[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "wireless-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11058 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████| 11058/11058 [00:19<00:00, 555.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['하루', '가', '각기', '가', '네요', '.'], ['무릎', '해', '드립니다', '.'], ['여행', '은데', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '괜찮', '죠', '.'], ['눈살', '그러', '찌푸려', '지', '죠', '.'], ['다시', '새로', '타', '는', '게', '마음', '편해요', '.'], ['다시', '새로이', '사', '는', '게', '마음', '편해요', '.'], ['많이', '모르', '고', '있', '을', '수', '도', '있', '어요', '.'], ['시간', '을', '정하', '고', '해', '보', 'ㅂ시오', '.'], ['시간', '을', '정해지', '고', '해', '보', '세요', '.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) ans_corpus Augmentation\n",
    "new_ans = []\n",
    "\n",
    "for idx in tqdm(range(len(ans_corpus2))):\n",
    "    old_src = ans_corpus2[idx]\n",
    "\n",
    "    new_src = lexical_sub(old_src, w2v)\n",
    "\n",
    "    if new_src is not None: new_ans.append(new_src)\n",
    "    else: new_ans.append(old_src)\n",
    "\n",
    "print(new_ans[:10])\n",
    "print(len(new_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "pretty-front",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11058"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "psychological-dominican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9697 9697\n",
      "[['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네']]\n",
      "[['하루', '가', '각기', '가', '네요', '.'], ['무릎', '해', '드립니다', '.'], ['여행', '은데', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '괜찮', '죠', '.'], ['눈살', '그러', '찌푸려', '지', '죠', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 \n",
    "que_corpus4 = []\n",
    "ans_corpus4 = []\n",
    "\n",
    "for i in range(len(new_ans)): \n",
    "    if new_ans[i] not in ans_corpus2: # 중복 제거 \n",
    "        que_corpus4.append(que_corpus2[i])\n",
    "        ans_corpus4.append(new_ans[i])\n",
    "        \n",
    "print(len(que_corpus4), len(ans_corpus4))\n",
    "print(que_corpus4[:5])\n",
    "print(ans_corpus4[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "distinct-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30099 30099\n"
     ]
    }
   ],
   "source": [
    "que_corpus = que_corpus2 + que_corpus3 + que_corpus4\n",
    "ans_corpus = ans_corpus2 + ans_corpus3 + ans_corpus4\n",
    "\n",
    "print(len(que_corpus), len(ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-coordinate",
   "metadata": {},
   "source": [
    "총 3만개의 데이터로 전체 데이터가 원래의 3배가량으로 늘어난 것을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-expression",
   "metadata": {},
   "source": [
    "# 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-episode",
   "metadata": {},
   "source": [
    "타겟 데이터인 ans_corpus 에 \\<start> 토큰과 \\<end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다.    \n",
    "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것이겠죠. 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다.\n",
    "    \n",
    "1) 타겟 데이터 전체에 \\<start> 토큰과 \\<end> 토큰을 추가해 줍니다.         \n",
    "2) 특수 토큰을 더함으로써 ans_corpus 또한 완성이 되었으니, que_corpus 와 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train 과 dec_train 을 얻습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "superior-colonial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<start>', '하루', '가', '또', '가', '네요', '.', '<end>'], ['<start>', '위로', '해', '드립니다', '.', '<end>'], ['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>'], ['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>'], ['<start>', '눈살', '이', '찌푸려', '지', '죠', '.', '<end>']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ans_corpus)):\n",
    "    ans_corpus[i] = [\"<start>\"] + ans_corpus[i] + [\"<end>\"]\n",
    "    \n",
    "print(ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-mentor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
