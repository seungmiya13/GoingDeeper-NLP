{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improved-shakespeare",
   "metadata": {},
   "source": [
    "GoingDeeper-NLP 14. BERT pretrained model 제작\n",
    "===\n",
    "\n",
    "가장 대표적인 pretrained ㅣlanguage model인 BERT의 pretrain 전과정을 진행해 보면서 BERT의 핵심원리를 깊이 이해해 본다.\n",
    "\n",
    "오늘은 일반적인 10M 정도의 작은 파라미터 사이즈의 BERT 모델을 만들어, 수백 MB 수준의 코퍼스 기반으로 pretrain 을 진행해 보도록 하겠습니다. 하지만 진행되는 과정은 정식 BERT와 동일할 테니 이를 토대로 pretrained model이 어떻게 만들어지는지를 경험해 봅니다. 모델을 만들고 학습시키는 것 이상으로 코퍼스 데이터를 가공해서 학습시켜야 할 task에 적합한 형태의 데이터셋으로 만들어가는 것이 큰 비중을 차지한다는 것을 알게 될 것입니다.\n",
    "    \n",
    "[목차]\n",
    "- 들어가며\n",
    "- 14.2 Tokenizer 준비\n",
    "- 14.3 데이터 전처리 (1) MASK 생성\n",
    "- 14.4 데이터 전처리 (2) NSP pair 생성\n",
    "- 14.5 데이터 전처리 (3) 데이터셋 완성\n",
    "- 14.6 BERT 모델 구현\n",
    "- 14.7 pretrain 진행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-nomination",
   "metadata": {},
   "source": [
    "# 14-2. Tokenizer 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-moses",
   "metadata": {},
   "source": [
    "BERT등의 pretrained model이 나오게 되었을 즈음 자연어처리 분야의 또 다른 중요한 흐름 중 하나는 BPE 등의 subword 기반의 토크나이징 기법이 주요한 방법론으로 굳어졌다는 점입니다. GPT의 BPE, BERT의 WordPiece 모델 등의 성공이 더욱 사람들에게 subword 기반의 토크나이저에 대한 확신을 주었습니다.\n",
    "\n",
    "오늘 우리는 SentencePiece 기반의 토크나이저를 준비하는 것으로 BERT pretrain 과정을 시작할 것입니다. 이 과정 자체는 이미 익숙하실 것이라 생각합니다.\n",
    "\n",
    "- [SentencePiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lovely-stress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-japanese",
   "metadata": {},
   "source": [
    "준비해 둔 한글 나무위키 코퍼스로부터 32000의 vocab_size를 갖는 sentencepiece 모델을 생성해 보겠습니다.\n",
    "\n",
    "BERT에 사용되는 [MASK], [SEP], [CLS] 등의 주요 특수문자가 vocab에 포함되어야 함에 주의해 주세요. 아래와 같이 모델을 생성하게 되면 약 30분 정도가 소요될 것입니다. 오래 기다리는 것이 힘드신 분은 미리 클라우드에 저장된 파일을 사용하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/GoingDeeper/nlp14/data/kowiki.txt'\n",
    "prefix = 'ko_32000'\n",
    "vocab_size = 32000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-effectiveness",
   "metadata": {},
   "source": [
    "sentencepiece 모델 학습이 끝난 후 생성된 ko_32000.model, ko_32000.vocab 두 파일은 커널이 생성되었을 때의 홈디렉토리에 생성되었을 것입니다. 이 두 파일을 ~/aiffel/bert_pretrain/models 아래로 이동시킨 후 계속 진행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eastern-geology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/GoingDeeper/nlp14/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/GoingDeeper/nlp14/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-contents",
   "metadata": {},
   "source": [
    "토크나이저가 잘 만들어졌는지 확인해 봅시다. 어떤 토큰이 만들어졌는지, 토크나이징 결과가 어떻게 나오는지 살펴볼까요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "round-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 token 7개를 제외한 나머지 tokens 들\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "streaming-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다', '었다', '▁지', '▁수', '▁19', '▁가', '▁시', '▁20', '▁기', '▁전', '▁아']\n"
     ]
    }
   ],
   "source": [
    "print(vocab_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "finite-clark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-mandate",
   "metadata": {},
   "source": [
    "토크나이저가 잘 작동하나요? 방금 우리는 SentencePiece 모델을 이용해 간단한 BERT의 Masked Language Model 학습용 데이터를 하나 생성해 보았습니다.\n",
    "\n",
    "다음 절부터 본격적으로 데이터 전처리 과정에 돌입하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-breeding",
   "metadata": {},
   "source": [
    "# 14-3. 데이터 전처리 (1) MASK 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-detector",
   "metadata": {},
   "source": [
    "BERT의 Masked Language Model은 GPT의 Next Token Prediction 태스크처럼 다음이 이어질 단어는? 을 맞추는 게 아니라 마스킹 된 다음 빈칸에 알맞은 단어는? 문제를 푸는 형식으로 구성됩니다. 이런 빈칸은 전체 토큰의 15% 정도가 적당하다고 합니다.\n",
    "\n",
    "이전 스텝의 Masked LM 데이터셋 예시에서 출발해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "desperate-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-highland",
   "metadata": {},
   "source": [
    "15%를 마스킹 한다고 해도 생각해 볼 것이 더 있습니다. Subword 기반으로 토크나이징을 했을 때 \\_대, [MASK], 민국이라고 가운데를 마스킹 했을 경우 해당 [MASK]가 '한'일 거라는 건 너무 쉽게 맞출 수 있습니다. '대한민국'이라는 패턴을 아주 자주 보게 될 테니까요.\n",
    "\n",
    "그래서 Masked LM 태스크를 구성할 땐 띄어쓰기 단위로 한꺼번에 마스킹해 주는 것이 좋습니다. 다음과 같이 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ambient-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] ['▁추적', '추', '적']\n",
      "[4] ['▁비가']\n",
      "[5] ['▁내리는']\n",
      "[6, 7, 8] ['▁날', '이었', '어']\n",
      "[9, 10] ['▁그날', '은']\n",
      "[11, 12, 13] ['▁', '왠', '지']\n",
      "[14, 15] ['▁손', '님이']\n",
      "[16] ['▁많아']\n",
      "[17] ['▁첫']\n",
      "[18] ['▁번에']\n",
      "[19, 20] ['▁삼', '십']\n",
      "[21] ['▁전']\n",
      "[22, 23] ['▁둘째', '번']\n",
      "[24, 25] ['▁오', '십']\n",
      "[26] ['▁전']\n",
      "[27, 28] ['▁오랜', '만에']\n",
      "[29, 30] ['▁받아', '보는']\n",
      "[31] ['▁십']\n",
      "[32, 33] ['▁전', '짜리']\n",
      "[34, 35, 36] ['▁백', '통', '화']\n",
      "[37, 38, 39] ['▁서', '푼', '에']\n",
      "[41] ['▁손바닥']\n",
      "[42, 43] ['▁위', '엔']\n",
      "[44, 45] ['▁기쁨', '의']\n",
      "[46, 47] ['▁눈', '물이']\n",
      "[48] ['▁흘러']\n",
      "[49, 50, 51] ['▁컬', '컬', '한']\n",
      "[52] ['▁목에']\n",
      "[53, 54] ['▁모', '주']\n",
      "[55, 56, 57] ['▁한', '잔', '을']\n",
      "[58, 59] ['▁적', '셔']\n",
      "[60] ['▁몇']\n",
      "[61] ['▁달']\n",
      "[62] ['▁포']\n",
      "[63] ['▁전부터']\n",
      "[64, 65, 66] ['▁콜', '록', '거리는']\n",
      "[67] ['▁아내']\n",
      "[68] ['▁생각에']\n",
      "[69, 70] ['▁그', '토록']\n",
      "[71] ['▁먹고']\n",
      "[72, 73] ['▁싶다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask 하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-kruger",
   "metadata": {},
   "source": [
    "- [startswith(str)](https://wikidocs.net/137643) : 지정한 문자열로 시작하면 True, 그렇지 않다면 False를 반환        \n",
    "- u\"\\u2581\" : ▁(위의 두꺼운 언더스코어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nasty-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18],\n",
       " [44, 45],\n",
       " [24, 25],\n",
       " [49, 50, 51],\n",
       " [31],\n",
       " [63],\n",
       " [41],\n",
       " [52],\n",
       " [22, 23],\n",
       " [71],\n",
       " [17],\n",
       " [19, 20],\n",
       " [60],\n",
       " [32, 33],\n",
       " [62],\n",
       " [46, 47],\n",
       " [29, 30],\n",
       " [72, 73],\n",
       " [6, 7, 8],\n",
       " [64, 65, 66],\n",
       " [67],\n",
       " [9, 10],\n",
       " [26],\n",
       " [55, 56, 57],\n",
       " [61],\n",
       " [34, 35, 36],\n",
       " [37, 38, 39],\n",
       " [21],\n",
       " [58, 59],\n",
       " [48],\n",
       " [69, 70],\n",
       " [4],\n",
       " [27, 28],\n",
       " [42, 43],\n",
       " [14, 15],\n",
       " [68],\n",
       " [5],\n",
       " [11, 12, 13],\n",
       " [1, 2, 3],\n",
       " [16],\n",
       " [53, 54]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-justice",
   "metadata": {},
   "source": [
    "개선된 Masking 로직을 다음과 같이 구현해 보았습니다. 마스킹 된 결과를 이전과 비교해 봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "experimental-checkout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '번', '[MASK]', '[MASK]', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁그레이트', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘러', '[MASK]', '[MASK]', '[MASK]', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0..1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-hotel",
   "metadata": {},
   "source": [
    "Masked LM의 라벨 데이터도 아래와 같이 생성하여 정리해 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "working-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [18, 24, 25, 31, 44, 45, 49, 50, 51, 63]\n",
      "mask_label : ['▁번에', '▁오', '십', '▁십', '▁기쁨', '의', '▁컬', '컬', '한', '▁전부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-stuart",
   "metadata": {},
   "source": [
    "## 🔶 create_pretrain_mask() : Masked LM을 위한 코퍼스 생성 메소드\n",
    "\n",
    "이번 스텝에서 구현할 최종 메소드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unauthorized-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-regard",
   "metadata": {},
   "source": [
    "create_pretrain_mask() 수행 결과를 다시 한번 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stainless-estate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '[MASK]', '[MASK]', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '[MASK]', '▁컬', '컬', '한', '[MASK]', '끊', '▁규장', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '[MASK]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [16, 17, 37, 38, 39, 48, 52, 53, 54, 67]\n",
      "mask_label : ['▁많아', '▁첫', '▁서', '푼', '에', '▁흘러', '▁목에', '▁모', '주', '▁아내']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-albert",
   "metadata": {},
   "source": [
    "# 14-4. 데이터 전처리 (2) NSP pair 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-calculator",
   "metadata": {},
   "source": [
    "BERT의 pretrain task로 Next Sentence Prediction이 있습니다. 문장 2개를 붙여 놓고 두 문장이 이어지는 것인지 아닌지 문장 호응관계를 맞출 수 있게 하는 것입니다.\n",
    "\n",
    "아래 문장을 예시로 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "automated-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "synthetic-disability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'],\n",
       " ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'],\n",
       " ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-technology",
   "metadata": {},
   "source": [
    "우선 원문에서 이어진 두 문장씩 짝지어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "liberal-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "celtic-native",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "tokens_a: 16 ['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아']\n",
      "tokens_b: 50 ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "tokens_a: 35 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가']\n",
      "tokens_b: 30 ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "tokens_a: 30 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날']\n",
      "tokens_b: 11 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-dealer",
   "metadata": {},
   "source": [
    "짝지은 두 문장을 그대로 두면 NSP task의 true label 케이스가 되고, 둘의 순서를 뒤바꾸면 false label 케이스가 되겠죠? 두 문장의 최대 길이를 유지하도록 trim을 적용한 후 50%의 확률로 true/false 케이스를 생성해 보겠습니다.\n",
    "\n",
    "- trim : 컴퓨터 프로그래밍에서 트리밍 또는 스트리핑은 문자열에서 선행 및 후행 공백을 제거하는 문자열 조작입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sexual-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "attempted-bubble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "is_next: 1\n",
      "tokens_a: 42 ['▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러']\n",
      "tokens_b: 19 ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "is_next: 0\n",
      "tokens_a: 37 ['만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']\n",
      "tokens_b: 24 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 32 ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-butter",
   "metadata": {},
   "source": [
    "이제 두 문장 사이에 segment 처리를 해주어야 합니다. 첫 번째 문장의 segment는 모두 0으로, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주는 것으로 마무리됩니다.\n",
    "\n",
    "이전 스텝의 create_pretrain_mask()까지 함께 호출되어 Mask LM용 데이터셋과 NSP용 데이터셋이 결합된 하나의 데이터셋으로 완성될 것입니다. BERT의 pretrain 은 두 가지 task가 동시에 수행되니까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "protecting-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "is_next: 1\n",
      "tokens_a: 8 ['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어']\n",
      "tokens_b: 53 ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포']\n",
      "tokens: 64 ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁방식으로', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '[MASK]', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '[MASK]', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[MASK]', '▁달', '▁포', '[SEP]']\n",
      "masked index: 9 [17, 19, 24, 27, 52, 55, 56, 57, 60]\n",
      "masked label: 9 ['▁많아', '▁번에', '▁번', '▁전', '▁목에', '▁한', '잔', '을', '▁몇']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "is_next: 1\n",
      "tokens_a: 16 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어']\n",
      "tokens_b: 45 ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간']\n",
      "tokens: 64 ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '욜', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁어머니와', '▁이와테', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁하시', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '[MASK]', '▁간', '[SEP]']\n",
      "masked index: 9 [11, 12, 13, 24, 25, 31, 52, 53, 61]\n",
      "masked label: 9 ['▁그릇', '을', '▁이제는', '▁떠올', '라', '▁가는', '▁나가', '고', '▁아내의']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 11 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "tokens_b: 30 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날']\n",
      "tokens: 44 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '[SEP]']\n",
      "segment: 44 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 44 ['[CLS]', '▁난', '▁맨', '날', '[MASK]', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '[MASK]', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '[MASK]', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '[MASK]', '[MASK]', '▁오늘', '은', '[MASK]', '▁좋은', '▁날', '[SEP]']\n",
      "masked index: 6 [4, 18, 28, 36, 37, 40]\n",
      "masked label: 6 ['▁이렇게', '▁비는', '▁아내가', '▁몰', '라', '▁운수']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        # tokens & aegment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "younger-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁방식으로', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '[MASK]', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '[MASK]', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[MASK]', '▁달', '▁포', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [17, 19, 24, 27, 52, 55, 56, 57, 60], 'mask_label': ['▁많아', '▁번에', '▁번', '▁전', '▁목에', '▁한', '잔', '을', '▁몇']}\n",
      "{'tokens': ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '욜', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁어머니와', '▁이와테', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁하시', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '[MASK]', '▁간', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 13, 24, 25, 31, 52, 53, 61], 'mask_label': ['▁그릇', '을', '▁이제는', '▁떠올', '라', '▁가는', '▁나가', '고', '▁아내의']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '[MASK]', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '[MASK]', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '[MASK]', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '[MASK]', '[MASK]', '▁오늘', '은', '[MASK]', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 18, 28, 36, 37, 40], 'mask_label': ['▁이렇게', '▁비는', '▁아내가', '▁몰', '라', '▁운수']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-membrane",
   "metadata": {},
   "source": [
    "## 🔶 create_pretrain_instances() : Next Sentence Prediction을 위한 코퍼스 생성 메소드\n",
    "\n",
    "이번 스텝에서 구현할 최종 메소드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "theoretical-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-understanding",
   "metadata": {},
   "source": [
    "create_pretrain_instances() 수행 결과를 다시 한번 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "whole-victim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁날', '이었', '어', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '[MASK]', '[MASK]', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '[MASK]', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '[MASK]', '[MASK]', '[MASK]', '▁적', '셔', '▁몇', '부에는', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 14, 15, 36, 50, 51, 52, 56], 'mask_label': ['▁그날', '은', '▁삼', '십', '▁손바닥', '▁한', '잔', '을', '▁달']}\n",
      "{'tokens': ['[CLS]', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '[MASK]', '[MASK]', '▁일찍', '이라도', '▁들어와', '껍', '▁무역을', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와', '[SEP]', '▁생각에', '▁그', '토록', '[MASK]', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '[MASK]', '▁살', '▁수', '꾹', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [25, 26, 30, 31, 42, 49, 50, 51, 54], 'mask_label': ['▁싶', '으면', '▁달라', '던', '▁먹고', '▁그릇', '을', '▁이제는', '▁있어']}\n",
      "{'tokens': ['[CLS]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '[MASK]', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '[MASK]', '[MASK]', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '[SEP]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '나는', '▁세기', '01', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [7, 19, 20, 40, 41, 42], 'mask_label': ['▁점점', '▁걱', '정은', '▁좋', '을', '까']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-contents",
   "metadata": {},
   "source": [
    "# 14-5. 데이터 전처리 (3) 데이터셋 완성\n",
    "\n",
    "이제 우리가 다루어야 할 kowiki.txt에 대해 본격적으로 들여다보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interstate-steam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/GoingDeeper/nlp14/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-blame",
   "metadata": {},
   "source": [
    "전체 라인 수가 확인되시나요? 거의 400만 개에 육박하는 수치입니다.\n",
    "\n",
    "위키 문서는 하나의 도큐먼트가 주제 키워드에 대해 상세 내용이 설명으로 따라붙어 있는 형태로 구성되어 있지요? 도큐먼트 주제별로 잘 나눠지는지도 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "actual-emerald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d13cc9e7024f5ca21548b1dba70103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지미', '▁카터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '▁대통령', '▁(19', '77', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인권', '과', '▁중재', '▁역할에', '▁대한', '▁공로를', '▁인정받아', '▁노벨', '▁평화', '상을', '▁받게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념을', '▁다루는', '▁학문이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용해서', '▁공', '리로', '▁구성된', '▁추상', '적', '▁구조를', '▁연구하는', '▁학문', '으로', '▁여겨', '지기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학문', '들과', '▁깊은', '▁연', '관을', '▁맺고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학의', '▁분야', '들과는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론을', '▁일반화', '▁및', '▁추상', '화', '시킬', '▁수', '▁있다는', '▁차이가', '▁있다고', '▁한다', '.', '▁수', '학자들은', '▁그러한', '▁개념', '들에', '▁대해서', '▁추측', '을', '▁하고', ',', '▁적절', '하게', '▁선택', '된', '▁정의', '와', '▁공리', '로부터의', '▁엄', '밀한', '▁연', '역을', '▁통해서', '▁추측', '들의', '▁진', '위를', '▁파악', '한다', '.']\n",
      "['▁수', '학의', '▁기초를', '▁확실히', '▁세우', '기', '▁위해', ',', '▁수리', '논', '리', '학과', '▁집합', '론이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범주', '론이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위기', '”', '라는', '▁말은', '▁대략', '▁1900', '년에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날에도', '▁계속되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논쟁', '에', '▁의해', '▁촉발', '되었으며', ',', '▁그', '▁논쟁', '에는', '▁칸', '토', '어의', '▁집합', '론과', '▁브라우', '어', '-', '힐', '베르트', '▁논쟁이', '▁포함되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상수']\n",
      "['▁수학에서', '▁상수', '란', '▁그', '▁값이', '▁변하지', '▁않는', '▁불변', '량으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리적', '▁측정', '과는', '▁상관없이', '▁정의된다', '.']\n",
      "['▁특정', '▁수학', '▁상수', ',', '▁예를', '▁들면', '▁골', '롬', '-', '딕', '맨', '▁상수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상수', '같은', '▁상', '수는', '▁다른', '▁수학', '상수', '▁또는', '▁함수', '와', '▁약한', '▁상관', '관계', '▁또는', '▁강한', '▁상관', '관계를', '▁갖는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언어를', '▁예술적', '▁표현의', '▁제', '재로', '▁삼아', '▁새로운', '▁의미를', '▁창출', '하여', ',', '▁인간과', '▁사회를', '▁진실', '되게', '▁묘사', '하는', '▁예술의', '▁하위', '분야', '이다', '.', '▁간단하게', '▁설명', '하면', ',', '▁언어를', '▁통해', '▁인간의', '▁삶을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형상', '화한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문예', '(', '文', '藝', ')', '라고', '▁부르는', '▁것이', '▁옳', '으며', ',', '▁문학을', '▁학문의', '▁대상', '으로서', '▁탐구', '하는', '▁학문의', '▁명칭', '▁역시', '▁문예', '학', '이다', '.', '▁문예', '학은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵심', '분야', '로서', '▁인문', '학의', '▁하위', '범', '주에', '▁포함된다', '.']\n",
      "['▁반영', '론적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품을', '▁창작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감상', '하는', '▁입장', '이고', ',', '▁내재', '적', '▁관', '점의', '▁감', '상은', '▁작품의', '▁형식', ',', '▁내용에', '▁국한', '하여', '▁감상', '하는', '▁것이다', '.', '▁표현', '론적', '▁관', '점의', '▁감', '상은', '▁작가의', '▁전기', '적', '▁사실과', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것이고', ',', '▁수용', '론적', '▁관', '점의', '▁감', '상은', '▁독', '자와', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라의', '▁각', '▁현황', '과', '▁주권', '▁승인', '▁정보를', '▁개', '요', '▁형태로', '▁나열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록에', '▁포함되지', '▁않은', '▁다음', '▁국가는', '▁몬테', '비', '데오', '▁협약', '의', '▁모든', '▁조건을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질의', '▁성질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그에', '▁수반', '하는', '▁에너지의', '▁변화를', '▁연구하는', '▁자연과', '학의', '▁한', '▁분야이다', '.', '▁물리학', '도', '▁역시', '▁물질을', '▁다루는', '▁학문', '이지만', ',', '▁물리학', '이', '▁원', '소와', '▁화합', '물을', '▁모두', '▁포함한', '▁물체의', '▁운동과', '▁에너지', ',', '▁열', '적', '·', '전기', '적', '·', '광', '학적', '·', '기계', '적', '▁속', '성을', '▁다루고', '▁이러한', '▁현상', '으로부터', '▁통일된', '▁이론을', '▁구축', '하려는', '▁것과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재하는', '▁물질을', '▁이용하여', '▁특정한', '▁목적에', '▁맞는', '▁새로운', '▁물질을', '▁합성', '하는', '▁길을', '▁제공하며', ',', '▁이는', '▁농작', '물의', '▁증', '산', ',', '▁질병의', '▁치료', '▁및', '▁예방', ',', '▁에너지', '▁효율', '▁증대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공한다', '.']\n",
      "['▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '물의', '▁범위가', '▁크게', '▁넓', '어져', '▁탄소', '▁사슬', '▁또는', '▁탄소', '▁고', '리를', '▁가진', '▁모든', '▁화합', '물을', '▁뜻한다', '.', '▁유기', '화', '학의', '▁오랜', '▁관심', '사는', '▁유기', '▁화합', '물의', '▁합성', '▁메커니즘', '이다', '.', '▁현대에', '▁들어서', '▁핵', '자기', '▁공명', '법과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-georgia",
   "metadata": {},
   "source": [
    "이전 스텝에서 완성했던 create_pretrain_instances()를 코퍼스에 적용할 수 있는지 몇 라인에 대해서만 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "southern-zealand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110cda4a40d04dc0bfc6993e89422443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '▁X', '선', '[MASK]', '[MASK]', '▁등이', '▁개발되어', '[MASK]', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 7, 25, 26, 27, 28, 36, 37], 'mask_label': ['▁결정', '학', '▁유기', '▁등도', '▁유기', '화', '학에서', '▁탄', '소로']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '[MASK]', '[MASK]', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 9, 10, 17, 18, 26, 27, 28], 'mask_label': ['▁유기', '▁화합물', '▁분석', '에', '▁플라스틱', ',', '▁유기', '화', '학에서']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '[MASK]', '[MASK]', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 25, 26, 27, 28, 41, 59, 60], 'mask_label': ['▁있어서', '▁매우', '▁등도', '▁유기', '화', '학에서', '▁연구하는', '▁뜻', '하였으나']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '[MASK]', '▁달린다', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '[MASK]', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 14, 47, 48, 49, 50, 51, 62], 'mask_label': ['▁개발되어', '▁유기', '▁방법으로', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁유기']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '[MASK]', '[MASK]', '▁유기', '[MASK]', '▁분석', '에', '[MASK]', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 6, 8, 11, 33, 34, 35, 59, 60], 'mask_label': ['▁등이', '▁개발되어', '▁화합물', '▁있어서', '▁유기', '화', '학은', '▁뜻', '하였으나']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '[MASK]', '[MASK]', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '[MASK]', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [15, 16, 25, 42, 43, 44, 45, 47, 62], 'mask_label': ['▁자리잡았다', '.', '▁등도', '▁분', '과', '이다', '.', '▁유기', '▁유기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '[MASK]', '[MASK]', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 39, 40, 52, 53, 59, 60, 62], 'mask_label': ['▁유기', '▁화합물', '▁화합', '물을', '▁동물', '로부터', '▁뜻', '하였으나', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '[MASK]', '[MASK]', '[MASK]', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '[MASK]', '[MASK]', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [8, 9, 10, 25, 38, 48, 49, 57, 58], 'mask_label': ['▁화합물', '▁분석', '에', '▁등도', '▁이루어진', '▁화합', '물은', '▁화합', '물을']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '[MASK]', '[MASK]', '[MASK]', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 19, 20, 21, 22, 23, 24, 25, 61], 'mask_label': ['▁중요한', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁지금은']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '[MASK]', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [15, 16, 42, 43, 44, 45, 46, 48, 49], 'mask_label': ['▁자리잡았다', '.', '▁분', '과', '이다', '.', '▁원래', '▁화합', '물은']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 11, 13, 26, 27, 28, 52, 53], 'mask_label': ['▁X', '선', '▁있어서', '▁중요한', '▁유기', '화', '학에서', '▁동물', '로부터']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '[MASK]', '[MASK]', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '[MASK]', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 12, 13, 36, 37, 41, 48, 49], 'mask_label': ['▁결정', '학', '▁매우', '▁중요한', '▁탄', '소로', '▁연구하는', '▁화합', '물은']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리 함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-gambling",
   "metadata": {},
   "source": [
    "# 🔶 make_pretrain_data() : BERT pretrain 데이터셋 생성 메소드\n",
    "\n",
    "전체 전처리 과정을 거쳐 최종적으로 만들어지는 BERT pretrain 데이터셋 생성 메소드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "flexible-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-sunday",
   "metadata": {},
   "source": [
    "이제 약 400만 라인에 해당하는 전체 코퍼스에 대해 make_pretrain_data()를 구동해 봅시다. 10여 분 가량 시간이 소요될 수 있습니다.\n",
    "최종적으로 생성된 데이터셋은 json 포맷으로 저장될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "noted-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/GoingDeeper/nlp14/data/bert_pre_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "weighted-modem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862285"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-conversation",
   "metadata": {},
   "source": [
    "데이터셋 파일을 만드는 것까지 수행되었습니다.\n",
    "\n",
    "하지만 여기서 고려해야 할 점이 있습니다. 우리가 다루어야 할 데이터셋은 사이즈가 큽니다. 만들어질 json 데이터파일의 크기가 1.4GB 정도 됩니다. 실제 BERT 학습용의 백 분의 일 사이즈 정도밖에 안 되겠지만 그럼에도 불구하고 이렇게 큰 파일을 로딩하는 함수를 만들 때는 메모리 사용량과 관련해 고려해야 할 점이 있습니다.\n",
    "\n",
    "그래서 우리는 np.memmap을 사용해서 메모리 사용량을 최소화하는 방법을 시도해 볼 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "incorrect-karen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-mapping",
   "metadata": {},
   "source": [
    "만들어진 json 파일을 라인 단위로 읽어 들여 np.memmap에 로딩해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bright-butter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b54fcc0bb6e4330baedccf5afbbb623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/862285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁태어났다', '.', '▁조지아', '▁공과', '대학교를', '▁졸업하였다', '.', '▁그', '[MASK]', '▁해군에', '▁들어가', '▁전함', '·', '원자', '력', '·', '잠', '수', '함의', '▁승무', '원으로', '▁일하였다', '.', '▁1953', '년', '▁미국', '▁해군', '▁대', '위로', '▁예편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈을', '▁벌', '었다', '.', '▁그의', '▁별명이', '[MASK]', '[MASK]', '[MASK]', '▁농부', '\"', '▁(', 'P', 'ean', 'ut', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁알려졌다', '.', '[SEP]', '▁늦', '되면서', '[MASK]', '▁주', '▁상원', '▁의원', '▁선거에서', '▁낙선', '하나', '▁그', '▁선거가', '▁부정', '선거', '▁', '였', '음을', '▁입증', '하게', '▁되어', '▁당선', '되고', ',', '▁1966', '년', '▁조지아', '▁주', '▁지사', '▁선거에', '▁낙선', '하지만', '[MASK]', '[MASK]', '▁조지아', '▁주', '▁지', '사를', '▁역임했다', '.', '[MASK]', '▁되기', '▁전', '▁조지아', '주', '▁상원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '[MASK]', '[MASK]', '▁조지아', '▁지', '사로', '▁근무했다', '.', '▁조지아', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [9, 48, 49, 50, 57, 58, 59, 60, 61, 65, 66, 67, 95, 96, 103, 118, 119, 126], 'mask_label': ['▁후', '▁\"', '땅', '콩', '▁F', 'ar', 'mer', ')', '로', '▁1962', '년', '▁조지아', '▁1970', '년', '▁대통령이', '▁1975', '년까지', '▁주지']}\n",
      "enc_token: [5, 1605, 27599, 5551, 14146, 15991, 8637, 27599, 13, 6, 25987, 2247, 15033, 27873, 14475, 27813, 27873, 28196, 27636, 10185, 16285, 1232, 22935, 27599, 4777, 27625, 243, 2780, 14, 1509, 22095, 414, 165, 1697, 28290, 27873, 27703, 27683, 593, 21, 29007, 399, 5540, 813, 17, 27599, 307, 16905, 6, 6, 6, 19041, 27718, 98, 27878, 15784, 2543, 6, 6, 6, 6, 6, 4578, 27599, 4, 4427, 1239, 6, 37, 11234, 2378, 5249, 9858, 3294, 13, 20590, 2386, 2163, 27596, 27671, 969, 8047, 173, 607, 2387, 317, 27604, 3926, 27625, 5551, 37, 18995, 8198, 9858, 1447, 6, 6, 5551, 37, 18, 451, 4267, 27599, 6, 6436, 25, 5551, 27646, 18205, 928, 157, 27821, 61, 27773, 530, 27604, 3372, 523, 6, 6, 5551, 18, 982, 13264, 27599, 5551, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0    81     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   103 28313 28290     0     0     0     0     0     0   309   337  5771\n",
      " 27616 27603     0     0     0  3715 27625  5551     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0  1921\n",
      " 27625     0     0     0     0     0     0  4864     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  3409   673\n",
      "     0     0     0     0     0     0  5053     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1976', '년', '▁대통령', '▁선거에', '▁민주당', '▁후보로', '▁출마하여', '▁도덕', '주의', '▁정책으로', '▁내세워', ',', '[MASK]', '[MASK]', '▁누르고', '▁당선되었다', '.', '▁카터', '▁대통령은', '▁에너지', '▁개발을', '▁촉구', '했으나', '▁공화', '당의', '▁반대로', '▁무산되었다', '.', '[SEP]', '▁카', '터는', '▁이집', '트와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠프', '▁데이비', '드에서', '▁안', '와', '르', '[MASK]', '[MASK]', '▁대통령과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중동', '[MASK]', '▁위한', '▁캠프', '데이', '비', '드', '▁협정을', '▁체결했다', '.', '▁그러나', '▁이것은', '▁공화', '당과', '▁미국의', '▁유대인', '[MASK]', '▁반발을', '▁일으켰다', '.', '▁1979', '년', '▁백악', '관에서', '▁양국', '▁간의', '▁평화', '조약', '으로', '[MASK]', '[MASK]', '[MASK]', '▁또한', '▁소련과', '▁제', '2', '차', '▁전략', '▁무기', '[MASK]', '▁협', '상에', '▁조인', '했다', '.', '▁카', '터는', '[MASK]', '[MASK]', '[MASK]', '▁당시', '[MASK]', '▁등', '▁인권', '▁후진', '국의', '▁국민들의', '▁인', '권을', '▁지키기', '▁위해', '▁노력', '했으며', ',', '[MASK]', '▁이후', '▁계속해서', '▁도덕', '정', '치를', '▁내세', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [13, 14, 45, 46, 51, 52, 57, 72, 73, 85, 86, 87, 95, 103, 104, 105, 107, 120], 'mask_label': ['▁포', '드를', '▁사다', '트', '▁베', '긴', '▁평화를', '▁단체의', '▁반발을', '▁이끌', '어졌다', '.', '▁제한', '▁1970', '년대', '▁후반', '▁대한민국', '▁취임']}\n",
      "enc_token: [5, 3306, 27625, 663, 8198, 4867, 4896, 19160, 6244, 238, 22033, 19990, 27604, 6, 6, 10071, 7965, 27599, 25250, 5906, 3634, 8085, 9747, 1003, 4460, 1547, 4771, 18474, 27599, 4, 207, 4612, 2703, 3604, 3426, 27607, 3358, 54, 27604, 10251, 3640, 3552, 172, 27665, 27699, 6, 6, 13799, 334, 27637, 29887, 271, 28099, 1011, 27644, 280, 8021, 6, 521, 10251, 4282, 27694, 27681, 15990, 19102, 27599, 330, 1487, 4460, 4040, 679, 7455, 6, 21408, 6564, 27599, 2995, 27625, 10312, 6749, 13195, 2714, 2793, 8993, 9, 6, 6, 6, 276, 23197, 30, 27619, 27751, 2835, 3841, 6, 617, 1824, 15876, 31, 27599, 207, 4612, 6, 6, 6, 316, 6, 50, 5636, 17092, 137, 18896, 42, 917, 15177, 231, 3375, 530, 27604, 6, 165, 6357, 6244, 27642, 1233, 5890, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   119  1486     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0  7025 27677     0\n",
      "     0     0     0   271 28099     0     0     0     0 14237     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 15747 21408     0     0     0     0     0     0     0     0     0     0\n",
      "     0  1435  2521 27599     0     0     0     0     0     0     0  1956\n",
      "     0     0     0     0     0     0     0  1921   596  1840     0   410\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  2659     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', ',', '▁1982', '년까지', '▁3', '단', '계에', '▁걸쳐', '▁주한미', '군을', '▁철수', '하기로', '▁했다', '.', '▁그러나', '▁주한미', '군', '사령', '부와', '▁정보', '기관', '·', '의', '회의', '[MASK]', '[MASK]', '▁부딪', '혀', '▁주한미', '군은', '▁완전', '철', '수', '▁대신', '▁6,000', '명을', '▁감축', '하는', '▁데', '▁그쳤다', '.', '▁또한', '▁박정희', '▁정권의', '▁인권', '▁문제', '▁등', '과의', '▁논란', '으로', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국을', '▁방문하여', '▁관계가', '▁다소', '▁회복', '되었다', '.', '[SEP]', '▁그러나', '[MASK]', '▁이란', '▁미국', '▁대사관', '[MASK]', '[MASK]', '▁사건에서', '▁인', '질', '[MASK]', '▁실패를', '▁이유로', '[MASK]', '[MASK]', '▁대통령', '▁선거에서', '▁공화', '당의', '▁로', '널드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패했다', '.', '▁또한', '▁임기', '[MASK]', '▁터', '진', '▁소련의', '▁아프가니스탄', '▁침공', '▁사건으로', '▁인해', '▁1980', '년', '[MASK]', '▁올림픽에', '▁반공', '국가', '들의', '[MASK]', '[MASK]', '[MASK]', '▁내세', '웠다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [24, 25, 50, 51, 52, 53, 73, 74, 77, 78, 82, 85, 86, 106, 116, 121, 122, 123], 'mask_label': ['▁반', '대에', '▁불', '협', '화', '음을', '▁주', '▁이란', '▁인', '질', '▁구출', '▁1980', '년', '▁말기에', '▁하계', '▁보이', '콧', '을']}\n",
      "enc_token: [5, 27604, 2760, 673, 49, 27737, 1949, 1633, 24438, 1262, 5337, 2390, 345, 27599, 330, 24438, 27722, 3069, 2576, 1071, 1468, 27873, 27601, 511, 6, 6, 11574, 28178, 24438, 941, 4626, 27917, 27636, 1083, 23453, 859, 26346, 38, 189, 11330, 27599, 276, 5298, 13574, 5636, 550, 50, 786, 2408, 9, 6, 6, 6, 6, 15139, 191, 27604, 2995, 27625, 125, 27662, 27, 27946, 27604, 16187, 14905, 4857, 5421, 3332, 43, 27599, 4, 330, 6, 3290, 243, 18590, 6, 6, 23937, 42, 27892, 6, 22684, 1827, 6, 6, 663, 5249, 4460, 1547, 194, 8631, 1169, 27803, 958, 113, 27596, 27944, 875, 174, 2087, 9510, 27599, 276, 11034, 6, 870, 27713, 5569, 7676, 3232, 6322, 751, 1640, 27625, 6, 5825, 13948, 4398, 247, 6, 6, 6, 5890, 1853, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   141   867     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0   128 27993 27683   969     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0    37  3290     0     0    42 27892     0     0     0 11560     0\n",
      "     0  1640 27625     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0 12145     0\n",
      "     0     0     0     0     0     0     0     0  2219     0     0     0\n",
      "     0  3052 28805 27607     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', ',', '▁박정희', '▁대통령이', '▁기묘', '韶', '▁중앙정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '▁대해', '▁그는', '▁이', '▁사건으로', '▁큰', '▁충격을', '▁받았으며', ',', '[MASK]', '[MASK]', '▁밴', '스', '▁국무', '장', '관을', '▁조', '문사', '절로', '▁파견했다', '.', '▁12', '·', '12', '[MASK]', '▁반란', '과', '▁5.', '17', '▁쿠데타', '에', '[MASK]', '▁초기에는', '▁강하게', '▁비난', '했으나', ',', '▁미국', '[MASK]', '▁신군', '부를', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁있었고', '▁결국', '▁묵', '인', '하는', '[MASK]', '▁태도를', '▁보이게', '▁됐다', '.', '[SEP]', '▁퇴임', '▁이후', '▁민간', '▁자원을', '▁적극', '▁활용한', '▁비영리', '▁기구', '인', '▁카터', '▁재', '단을', '▁설립한', '[MASK]', '[MASK]', '▁실현', '을', '▁위해', '▁제', '▁3', '세계의', '[MASK]', '[MASK]', '▁활동', '▁및', '▁기니', '▁벌', '레', '에', '▁의한', '▁드라', '쿤', '쿠르', '스', '▁질병', '▁방', '재를', '▁장안', '▁힘썼다', '.', '▁미국의', '▁빈곤', '층', '▁지원', '▁활동', ',', '▁사랑의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '▁분쟁', '▁중재', '▁등의', '▁활동도', '▁했다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 21, 22, 36, 43, 50, 53, 54, 55, 56, 62, 77, 81, 82, 89, 90, 105], 'mask_label': ['▁김재', '규', '▁사이', '러스', '▁군사', '▁대해', '▁정부가', '▁설득', '하는데', ',', '▁한계가', '▁듯한', '▁카터', '▁뒤', '▁민주주의', '▁선거', '▁감시', '▁위해']}\n",
      "enc_token: [5, 27604, 5298, 4864, 27387, 31931, 18525, 27638, 1312, 355, 2591, 27711, 2057, 433, 202, 8, 6322, 459, 10688, 5325, 27604, 6, 6, 1228, 27626, 4444, 27651, 1657, 53, 27181, 17544, 26520, 27599, 196, 27873, 1335, 6, 2342, 27644, 11262, 1695, 14078, 27600, 6, 6797, 7015, 3560, 1003, 27604, 243, 6, 26826, 1191, 6, 6, 6, 6, 2492, 875, 5374, 27628, 38, 6, 11162, 16915, 3842, 27599, 4, 13826, 165, 3174, 17304, 2929, 20639, 16068, 6673, 27628, 25250, 174, 1574, 7301, 6, 6, 5031, 27607, 231, 30, 49, 21655, 6, 6, 375, 228, 18137, 813, 27740, 27600, 1332, 17378, 28956, 16453, 27626, 5225, 95, 4445, 15108, 19607, 27599, 679, 14197, 28083, 770, 375, 27604, 14003, 313, 28333, 27614, 887, 27604, 605, 4476, 13267, 507, 27328, 345, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0  9918 27958     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   328  2086     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  1250     0     0     0     0     0     0   433     0     0     0     0\n",
      "     0     0  3840     0     0  5523  1294 27604 20984     0     0     0\n",
      "     0     0 10180     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0 25250     0     0     0   339  9889     0\n",
      "     0     0     0     0     0   822  7049     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   231     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁카터', '▁행정부', '▁이후', '▁미국이', '▁북', '핵', '▁위기', ',', '▁코소보', '▁전쟁', ',', '▁이라크', '▁전쟁과', '▁같이', '▁미국이', '[MASK]', '[MASK]', '▁최후', '로', '▁선택하는', '▁전통적', '▁사고를', '▁버리고', '▁나이에', '▁행동을', '▁선행', '하는', '▁행위에', '▁대해', '▁깊은', '▁유', '감을', '▁표시', '▁하며', '▁미국의', '▁군사적', '▁활동에', '▁강한', '▁반대', '▁입장을', '▁보이고', '▁있다', '.', '[SEP]', '▁특히', '▁국제', '▁분쟁', '[MASK]', '▁위해', '▁북한의', '▁김일성', ',', '▁아이', '티의', '▁세', '드', '라스', '▁장군', ',', '▁팔', '레인', '스타', '인의', '[MASK]', '[MASK]', '[MASK]', '▁보스', '니아의', '▁세르비아', '계', '▁정권', '▁같이', '▁미국', '▁정부에', '▁대해', '▁협상을', '▁거부', '하면서', '▁사태', '의', '▁위기를', '▁초래', '한', '▁인물', '▁및', '▁단체를', '▁직접', '[MASK]', '▁분쟁', '의', '[MASK]', '▁근본적으로', '▁해결하기', '▁위해', '▁힘썼다', '.', '▁이', '▁과정에서', '▁미국', '▁행정', '부와', '▁갈등을', '[MASK]', '▁했지만', ',', '▁전직', '▁대통령의', '▁권한', '과', '[MASK]', '[MASK]', '[MASK]', '▁인사', '들의', '[MASK]', '[MASK]', '[MASK]', '▁나갔다', '.', '▁1978', '년에', '▁채', '결', '된', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 18, 19, 26, 50, 66, 67, 68, 90, 93, 105, 112, 113, 114, 117, 118, 119], 'mask_label': ['▁카', '터는', '▁군사적', '▁행동을', '▁군사적', '▁조정을', '▁하', '마스', ',', '▁만나', '▁원인을', '▁보이기도', '▁재', '야', '▁유명', '▁활약으로', '▁해결', '해']}\n",
      "enc_token: [5, 6, 6, 25250, 21862, 165, 8424, 251, 28166, 10622, 27604, 26202, 506, 27604, 6157, 17305, 733, 8424, 6, 6, 12241, 27603, 26028, 15644, 13098, 8275, 7441, 5616, 16374, 38, 19690, 433, 4508, 46, 2196, 2466, 1368, 679, 9641, 8253, 2632, 1216, 5168, 7010, 28, 27599, 4, 698, 605, 4476, 6, 231, 9305, 11444, 27604, 520, 10694, 74, 27681, 1951, 4379, 27604, 961, 5346, 936, 692, 6, 6, 6, 6076, 4174, 4543, 27704, 5752, 733, 243, 6633, 433, 12149, 2324, 421, 4597, 27601, 11239, 8200, 27612, 1178, 228, 19762, 1069, 6, 4476, 27601, 6, 24806, 13425, 231, 19607, 27599, 8, 2208, 243, 895, 2576, 12627, 6, 3379, 27604, 5605, 5744, 3753, 27644, 6, 6, 6, 3329, 247, 6, 6, 6, 9946, 27599, 3331, 169, 481, 27783, 27711, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0   207  4612     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0  9641  5616     0     0     0     0\n",
      "     0     0  9641     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0 23144     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0    27  3678 27604     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0  2142     0     0 13635     0     0\n",
      "     0     0     0     0     0     0     0     0     0 23828     0     0\n",
      "     0     0     0     0   174 27775   939     0     0 17462  2317 27645\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁북한에', '▁대한', '▁미국의', '▁군사적', '▁행동이', '▁임', '박', '했으나', ',', '[MASK]', '▁전직', '▁대통령', '으로는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁미국과', '▁북', '[MASK]', '▁중재', '에', '▁큰', '▁기여를', '▁해', '▁위기를', '▁해결', '했다는', '[MASK]', '▁받았다', '.', '▁또한', '▁이', '▁때', '[MASK]', '▁대통령과', '맏', '▁주', '석의', '▁만남', '을', '▁주선', '했다', '.', '▁하지만', '▁그로부터', '▁수', '주일', '▁후', '▁김일', '성이', '▁갑자기', '▁사망하여', '▁김일', '성과', '▁김영삼', '의', '▁정상회담', '은', '▁이루어지지', '▁못했다', '.', '[SEP]', '▁미국의', '▁관', '타나', '모', '▁수용', '소', '▁문제', ',', '▁세계의', '▁인권', '문제', '에서도', '▁관심이', '▁깊', '어', '▁유엔', '에', '▁유엔', '인권', '고등', '판', '무', '관의', '▁제도를', '▁시행', '하도록', '▁노력', '하여', '▁독재', '자들의', '▁인권', '▁유', '린', '에', '▁대해', '▁제', '약을', '▁하고', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁만드는', '▁데', '▁기여', '하여', '▁독재', '자들', '▁같은', '▁인권', '유', '린', '범죄', '자를', '▁재판', '소로', '▁회', '부', '하여', '▁국제적인', '▁처벌을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [10, 14, 15, 16, 17, 18, 21, 30, 31, 32, 35, 36, 38, 65, 104, 105, 106, 107], 'mask_label': ['▁미국', '▁처음으로', '▁북한', '을', '▁방문', '하고', '▁양국의', '▁평가를', '▁받았다', '.', '▁때', '▁김영삼', '▁김일성', '▁미국의', '▁국제', '형사', '재판', '소를']}\n",
      "enc_token: [5, 25086, 92, 679, 9641, 18507, 273, 27914, 1003, 27604, 6, 5605, 663, 1030, 6, 6, 6, 6, 6, 5672, 251, 6, 13267, 27600, 459, 13856, 87, 11239, 2317, 2351, 6, 772, 27599, 276, 8, 84, 6, 13799, 29164, 37, 5361, 11842, 27607, 25754, 31, 27599, 589, 14313, 19, 10106, 81, 4636, 684, 5908, 26809, 4636, 1693, 9133, 27601, 27509, 27613, 13475, 2041, 27599, 4, 679, 88, 8011, 27716, 2237, 27688, 550, 27604, 5467, 5636, 5515, 643, 8181, 1910, 27633, 3708, 27600, 3708, 12972, 5059, 27841, 27725, 2429, 6520, 2404, 1816, 3375, 54, 7559, 2653, 5636, 46, 27870, 27600, 433, 30, 3297, 644, 27604, 6, 6, 6, 6, 3002, 189, 2187, 54, 7559, 3989, 226, 5636, 27690, 27870, 8822, 690, 2474, 2661, 270, 27638, 54, 12835, 19956, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0   243     0\n",
      "     0     0  1307  1876 27607  2017    48     0     0 25764     0     0\n",
      "     0     0     0     0     0     0  4549   772 27599     0     0    84\n",
      "  9133     0 11444     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0   679     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0   605 17905  4731  1358\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-customer",
   "metadata": {},
   "source": [
    "# 🔶 load_pre_train_data() : 학습에 필요한 데이터를 로딩하는 함수\n",
    "\n",
    "np.memmap을 사용해 메모리 효율적으로 만들어진 데이터를 로딩하는 함수를 아래와 같이 구성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "exotic-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "educational-weather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0def17dc8b3418ba77327764b1bfc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "destroyed-charleston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([    5,  1605, 27599,  5551, 14146, 15991,  8637, 27599,    13,\n",
       "             6, 25987,  2247, 15033, 27873, 14475, 27813, 27873, 28196,\n",
       "         27636, 10185, 16285,  1232, 22935, 27599,  4777, 27625,   243,\n",
       "          2780,    14,  1509, 22095,   414,   165,  1697, 28290, 27873,\n",
       "         27703, 27683,   593,    21, 29007,   399,  5540,   813,    17,\n",
       "         27599,   307, 16905,     6,     6,     6, 19041, 27718,    98,\n",
       "         27878, 15784,  2543,     6,     6,     6,     6,     6,  4578,\n",
       "         27599,     4,  4427,  1239,     6,    37, 11234,  2378,  5249,\n",
       "          9858,  3294,    13, 20590,  2386,  2163, 27596, 27671,   969,\n",
       "          8047,   173,   607,  2387,   317, 27604,  3926, 27625,  5551,\n",
       "            37, 18995,  8198,  9858,  1447,     6,     6,  5551,    37,\n",
       "            18,   451,  4267, 27599,     6,  6436,    25,  5551, 27646,\n",
       "         18205,   928,   157, 27821,    61, 27773,   530, 27604,  3372,\n",
       "           523,     6,     6,  5551,    18,   982, 13264, 27599,  5551,\n",
       "             6,     4], dtype=int32),\n",
       " memmap([    5,  2191, 27599,  5078,    81, 27604,   342,  4812, 27625,\n",
       "           294, 14456,     6, 24930,  2540, 27600,   488,  4871,  2524,\n",
       "         13358,   171, 27599,   330, 27604, 14456,     6,  4842, 27682,\n",
       "         27625,  2131, 14135,  9943,   761, 28254,   658,   171, 27599,\n",
       "            13,     6, 13069, 11312,   496,  8020,  1910, 27633,  4216,\n",
       "           664,   926,  5238,  1053,  5583, 27614,  2419,  2470, 27604,\n",
       "         21622, 27602,   838, 15403, 27760,   824,  3525, 13998,  7619,\n",
       "         27599,     4,   371, 27604,    29, 28018, 27793,  1326, 27787,\n",
       "            66,   412, 22289,    28, 27599,  1326,  5884,     6,  2573,\n",
       "            66,  1599, 27653,   639,  3883,   353, 27599, 17506,  5263,\n",
       "          1326,  5884,    19,   402,     6,     6,     6,     6,     6,\n",
       "          2742,    31, 27599,     6, 16343,     6,     6,     6,     6,\n",
       "             6,     6,     6,     6, 10603, 27616,     9, 25545,  1599,\n",
       "         27653,   639,   254,   238,   787,  2654,   784, 27604,  1925,\n",
       "           259,     4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            81,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,   103, 28313, 28290,     0,     0,     0,\n",
       "             0,     0,     0,   309,   337,  5771, 27616, 27603,     0,\n",
       "             0,     0,  3715, 27625,  5551,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,  1921, 27625,     0,     0,\n",
       "             0,     0,     0,     0,  4864,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  3409,   673,     0,     0,     0,     0,     0,     0,\n",
       "          5053,     0], dtype=int32),\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,    89,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 15170,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  4031,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 11497,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,   231, 11497,  9933, 27607,  2042,\n",
       "             0,     0,     0, 11364,     0,  1911, 27604,    68, 27650,\n",
       "         27617,  3065, 28116, 27601,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], dtype=int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-happiness",
   "metadata": {},
   "source": [
    "# 14-6. BERT 모델 구현"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "human-blade",
   "metadata": {},
   "source": [
    "이제 본격적으로 BERT model을 구현해 보겠습니다.\n",
    "\n",
    "BERT가 transformer encoder로 구현되어 있다는 것은 잘 알고 계시리라 생각합니다. 이미 여러 번 다뤄보셨을 transformer의 모델 구조와 거의 유사하지만, 아래 그림과 같이 3개의 embedding 레이어를 가진다는 점에 유의해야 합니다.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "우선 몇 가지 유틸리티 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "derived-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "seeing-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "emotional-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "southeast-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sapphire-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "crucial-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-backing",
   "metadata": {},
   "source": [
    "이제 본격적으로 embedding 레이어를 쌓아나가겠습니다. 아래는 Token Embedding의 구현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "under-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-failing",
   "metadata": {},
   "source": [
    "Positional Embedding 레이어는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "stuffed-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-battlefield",
   "metadata": {},
   "source": [
    "상대적으로 매우 간단한 Segment Embedding은 별도의 레이어를 구현하지 않고 BERT 클래스에서 간단히 포함하도록 하겠습니다.\n",
    "\n",
    "아래는 자주 보았던 ScaleDotProductAttention과 이를 활용한 MultiHeadAttention입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "based-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hidden-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-association",
   "metadata": {},
   "source": [
    "이를 바탕으로 transformer encoder 레이어를 구성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "incident-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "funny-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-template",
   "metadata": {},
   "source": [
    "이제 다 왔습니다.\n",
    "\n",
    "최종적으로 구성할 BERT 레이어는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "marked-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-island",
   "metadata": {},
   "source": [
    "BERT 레이어를 바탕으로 최종적으로 만들어질 pretrain용 BERT 모델 구성은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "connected-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fleet-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-alliance",
   "metadata": {},
   "source": [
    "아주 작은 pretrain용 BERT 모델(test_model)을 생성하여 동작을 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "welcome-allowance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "macro-action",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 4s 27ms/step - loss: 11.1932 - nsp_loss: 0.7411 - mlm_loss: 10.4521 - nsp_acc: 0.6000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 10.0369 - nsp_loss: 0.6172 - mlm_loss: 9.4197 - nsp_acc: 0.8000 - mlm_acc: 0.0133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6b143f9510>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-prison",
   "metadata": {},
   "source": [
    "test_model.fit()이 잘 구동되나요?\n",
    "\n",
    "다음 스텝에서 본격적으로 학습을 진행해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-hardwood",
   "metadata": {},
   "source": [
    "# 14-7. pretrain 진행\n",
    "\n",
    "loss나 accuracy같이 기본적으로 필요한 계산 함수를 미리 정의해 둡시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faced-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "express-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-basketball",
   "metadata": {},
   "source": [
    "Learning Rate 스케줄링도 아래와 같이 구현합니다. WarmUp 이후 consine 형태로 감소하는 스케줄을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "married-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "official-disability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-kidney",
   "metadata": {},
   "source": [
    "이제 모델을 실제로 빌드 해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "committed-allowance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 10629632    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 32007)  0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 10,695,936\n",
      "Trainable params: 10,695,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-infrastructure",
   "metadata": {},
   "source": [
    "이제 본격적으로 학습을 진행합니다. 1Epoch만 학습하는 데도 10분 이상의 상당한 시간이 소요될 것입니다. 그리고 메모리 오류가 날 수 있으니 배치 사이즈에도 유의해 주세요. 참고로 우리는 전체 데이터셋 중의 1/7 수준인 128000건만 로딩해서 사용 중이라는 것을 기억합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "persistent-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 6000\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "computational-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[8192,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/bert/weight_shared_embedding/MatMul (defined at <ipython-input-42-ab0900313a6c>:61) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/model_1/bert/embedding_3/embedding_lookup/Reshape/_44]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[8192,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/bert/weight_shared_embedding/MatMul (defined at <ipython-input-42-ab0900313a6c>:61) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_100067]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_1/bert/weight_shared_embedding/MatMul:\n model_1/bert/weight_shared_embedding/Reshape (defined at <ipython-input-42-ab0900313a6c>:60)\n\nInput Source operations connected to node model_1/bert/weight_shared_embedding/MatMul:\n model_1/bert/weight_shared_embedding/Reshape (defined at <ipython-input-42-ab0900313a6c>:60)\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-1a6d7d8b5678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_dir}/bert_pre_train.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mlm_lm_acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_train_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_train_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[8192,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/bert/weight_shared_embedding/MatMul (defined at <ipython-input-42-ab0900313a6c>:61) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/model_1/bert/embedding_3/embedding_lookup/Reshape/_44]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[8192,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/bert/weight_shared_embedding/MatMul (defined at <ipython-input-42-ab0900313a6c>:61) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_100067]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_1/bert/weight_shared_embedding/MatMul:\n model_1/bert/weight_shared_embedding/Reshape (defined at <ipython-input-42-ab0900313a6c>:60)\n\nInput Source operations connected to node model_1/bert/weight_shared_embedding/MatMul:\n model_1/bert/weight_shared_embedding/Reshape (defined at <ipython-input-42-ab0900313a6c>:60)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
